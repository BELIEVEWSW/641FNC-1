{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "tf_config = tf.ConfigProto()\n",
    "tf_config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config=tf_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.tokenize import word_tokenize\n",
    "from keras.preprocessing.text import text_to_word_sequence, Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.optimizers import Optimizer\n",
    "from keras import callbacks\n",
    "from keras.layers import Input, Dense, LSTM, Embedding, Dropout, BatchNormalization, Activation, Bidirectional,concatenate\n",
    "from keras.models import Model\n",
    "import matplotlib as mpl\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "from keras.utils import plot_model \n",
    "from keras.optimizers import Adam\n",
    "from IPython.display import Image\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import np_utils\n",
    "\n",
    "np.random.seed(17)\n",
    "\n",
    "\n",
    "import sys\n",
    "import csv\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from feature_engineering import refuting_features, polarity_features, hand_features, gen_or_load_feats\n",
    "from feature_engineering import word_overlap_features\n",
    "from utils.dataset import DataSet\n",
    "from utils.generate_test_splits import kfold_split, get_stances_for_folds\n",
    "from utils.score import report_score, LABELS, score_submission\n",
    "from csv import DictReader\n",
    "from csv import DictWriter\n",
    "import codecs\n",
    "\n",
    "from utils.system import parse_params, check_version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_bodies = pd.read_csv('./fnc-1/train_bodies.csv')\n",
    "    \n",
    "train_stances = pd.read_csv('./fnc-1/train_stances.csv')\n",
    "\n",
    "test_bodies = pd.read_csv('./fnc-1/competition_test_bodies.csv')\n",
    "\n",
    "test_stances = pd.read_csv('./fnc-1/competition_test_stances.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge and concate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_stances['Stance'].replace('agree',0,True)\n",
    "train_stances['Stance'].replace('disagree',1,True)\n",
    "train_stances['Stance'].replace('discuss',2,True)\n",
    "train_stances['Stance'].replace('unrelated',3,True)\n",
    "train_merge = pd.merge(train_stances, train_bodies, on='Body ID')\n",
    "\n",
    "\n",
    "test_stances['Stance'].replace('agree',0,True)\n",
    "test_stances['Stance'].replace('disagree',1,True)\n",
    "test_stances['Stance'].replace('discuss',2,True)\n",
    "test_stances['Stance'].replace('unrelated',3,True)\n",
    "test_merge = pd.merge(test_stances, test_bodies, on='Body ID')\n",
    "\n",
    "test_merge.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SENT_LEN = 16\n",
    "MAX_VOCAB_SIZE = 50000\n",
    "LSTM_DIM = 100\n",
    "EMBEDDING_DIM = 50\n",
    "BATCH_SIZE = 128\n",
    "N_EPOCHS = 1\n",
    "MAX_SENT_LEN1 = 100\n",
    "MAX_VOCAB_SIZE1 = 50000\n",
    "\n",
    "\n",
    "train_merge['Headline'].head(10)\n",
    "\n",
    "test_merge['Headline'] = test_merge['Headline'].apply(lambda x: str(x))\n",
    "train_merge['Headline'] = train_merge['Headline'].apply(lambda x: str(x))\n",
    "\n",
    "test_merge['articleBody'] = test_merge['articleBody'].apply(lambda x: str(x))\n",
    "train_merge['articleBody'] = train_merge['articleBody'].apply(lambda x: str(x))\n",
    "\n",
    "\n",
    "train_merge = train_merge.sample(frac=1, random_state=10)\n",
    "\n",
    "\n",
    "word_seq_train_stances = [text_to_word_sequence(head) for head in train_merge['Headline']]\n",
    "word_seq_train_bodies = [text_to_word_sequence(body) for body in train_merge['articleBody']]\n",
    "\n",
    "\n",
    "\n",
    "word_seq_test_stances = [text_to_word_sequence(head) for head in test_merge['Headline']]\n",
    "word_seq_test_bodies = [text_to_word_sequence(body) for body in test_merge['articleBody']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a list for embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_seq = []\n",
    "\n",
    "for i in range(len(word_seq_train_stances)):\n",
    "    word_seq.append(word_seq_train_stances[i])\n",
    "for i in range(len(word_seq_train_bodies)):\n",
    "    word_seq.append(word_seq_train_bodies[i])\n",
    "\n",
    "\n",
    "for i in range(len(word_seq_test_stances)):\n",
    "    word_seq.append(word_seq_test_stances[i])\n",
    "for i in range(len(word_seq_test_bodies)):\n",
    "    word_seq.append(word_seq_test_bodies[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print('75th Percentile Sentence Length:', np.percentile([len(seq) for seq in word_seq_train_stances], 75))\n",
    "print('75th Percentile Sentence Length:', np.percentile([len(seq) for seq in word_seq_train_bodies], 75))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pass the words through tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_list = '\\r\\t\\n'\n",
    "tokenizer = Tokenizer(num_words=MAX_VOCAB_SIZE,filters=filter_list)\n",
    "tokenizer.fit_on_texts([seq for seq in word_seq])\n",
    "print(\"Number of words in vocabulary:\", len(tokenizer.word_index))\n",
    "\n",
    "tokenizer.word_index\n",
    "\n",
    "X = tokenizer.texts_to_sequences([' '.join(seq[:MAX_SENT_LEN]) for seq in word_seq_train_stances])\n",
    "X = pad_sequences(X, maxlen=MAX_SENT_LEN, padding='post', truncating='post')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1 = tokenizer.texts_to_sequences([' '.join(seq[:MAX_SENT_LEN1]) for seq in word_seq_train_bodies])\n",
    "X1 = pad_sequences(X1, maxlen=MAX_SENT_LEN1, padding='post', truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_bodies = tokenizer.texts_to_sequences([' '.join(seq[:MAX_SENT_LEN1]) for seq in word_seq_test_bodies])\n",
    "X_test_bodies = pad_sequences(X_test_bodies, maxlen=MAX_SENT_LEN1, padding='post', truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_bodies[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_stances = tokenizer.texts_to_sequences([' '.join(seq[:MAX_SENT_LEN]) for seq in word_seq_test_stances])\n",
    "X_test_stances = pad_sequences(X_test_stances, maxlen=MAX_SENT_LEN, padding='post', truncating='post')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split train test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = train_merge['Stance']\n",
    "\n",
    "encoder_train = LabelEncoder()\n",
    "encoder_train.fit(y)\n",
    "encoded_train = encoder_train.transform(y)\n",
    "# convert integers to dummy variables (i.e. one hot encoded)\n",
    "dummy_y_train = np_utils.to_categorical(encoded_train)\n",
    "\n",
    "y_test = test_merge['Stance']\n",
    "\n",
    "\n",
    "\n",
    "train_size =math.ceil( X.shape[0] * 0.9)\n",
    "train_size_end =math.ceil( X.shape[0] * 0.2)\n",
    "\n",
    "X_train = X[0:train_size]\n",
    "X1_train = X1[0:train_size]\n",
    "\n",
    "X_val = X[train_size:]\n",
    "X1_val = X1[train_size:]\n",
    "\n",
    "y_train = dummy_y_train[0:train_size]\n",
    "y_val = dummy_y_train[train_size:]\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X1_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "print(X_val.shape)\n",
    "print(X1_val.shape)\n",
    "print(y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import glove embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "\n",
    "glove_input_file = 'd:/641/glove.twitter.27B.50d.txt'\n",
    "word2vec_output_file = 'glove.500d.txt.word2vec'\n",
    "glove2word2vec(glove_input_file, word2vec_output_file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an embedding matrix containing only the word's in our vocabulary\n",
    "# If the word does not have a pre-trained embedding, then randomly initialize the embedding\n",
    "embeddings = KeyedVectors.load_word2vec_format(word2vec_output_file, binary=False)\n",
    "embeddings_matrix = np.random.uniform(-0.05, 0.05, size=(len(tokenizer.word_index)+1, EMBEDDING_DIM)) # +1 is because the matrix indices start with 0\n",
    "\n",
    "for word, i in tokenizer.word_index.items(): # i=0 is the embedding for the zero padding\n",
    "    try:\n",
    "        embeddings_vector = embeddings[word]\n",
    "    except KeyError:\n",
    "        embeddings_vector = None\n",
    "    if embeddings_vector is not None:\n",
    "        embeddings_matrix[i] = embeddings_vector\n",
    "        \n",
    "del embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate 2 models for headline and article body seperately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model2():\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=len(tokenizer.word_index)+1,\n",
    "                          output_dim=EMBEDDING_DIM,\n",
    "                          weights = [embeddings_matrix], \n",
    "                          input_length=MAX_SENT_LEN,\n",
    "                          trainable=False, name='word_embedding_layer', #False\n",
    "                          mask_zero=True))\n",
    "\n",
    "    model.add(Dense(50,  name='dense_layer'))\n",
    "\n",
    "    model.add(Dense(25, name='output_layer1'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model3():\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=len(tokenizer.word_index)+1,\n",
    "                          output_dim=EMBEDDING_DIM,\n",
    "                          input_length=MAX_SENT_LEN1,\n",
    "                          weights = [embeddings_matrix], trainable=False, name='word_embedding_layer1', #False\n",
    "                          mask_zero=True))\n",
    "    model.add(Dense(50, name='dense_layer1'))\n",
    "    model.add(Dense(25,  name='output_layer2'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concate the output then pass through a BiLSTM layer, dropout layer and dense layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_stance1 = create_model2()\n",
    "model_bodies1 = create_model3()\n",
    "\n",
    "combinedInput =keras.layers.Concatenate(axis=1)([model_stance1.output, model_bodies1.output])\n",
    "\n",
    "x = Bidirectional(LSTM(LSTM_DIM, return_sequences=False, name='Bidrectional_lstm1'))(combinedInput)\n",
    "x = Dropout(rate=0.2, name='dropout_2')(x)\n",
    "x = Dense(4, activation=\"softmax\")(x)\n",
    "model = Model(inputs=[model_stance1.input, model_bodies1.input], outputs=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'] )\n",
    "\n",
    "filepath=\"divide_then_merge_textsummarization_{epoch:02d}_{val_loss:.4f}.h5\"\n",
    "checkpoint = callbacks.ModelCheckpoint(filepath, \n",
    "                                       monitor='val_loss', \n",
    "                                       verbose=0, \n",
    "                                       save_best_only=False)\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_model(model, to_file='bilstm_concate_model.png', show_layer_names=True, show_shapes=True)\n",
    "Image('bilstm_concate_model.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history  = model.fit(\n",
    "    [X_train, X1_train], y_train,\n",
    "    validation_data=([X_val, X1_val], y_val),\n",
    "    epochs= 40, batch_size=BATCH_SIZE ,verbose = 1,callbacks = callbacks_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weighted score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = [LABELS[int(np.argmax(a, axis = -1))] for a in model.predict([X_test_stances,X_test_bodies])]\n",
    "\n",
    "actual = [LABELS[int(a)] for a in y_test]\n",
    "\n",
    "report_score(actual,predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# accuracy and loss plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
